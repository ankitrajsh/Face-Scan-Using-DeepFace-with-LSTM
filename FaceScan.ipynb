{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install DeepFace\n"
      ],
      "metadata": {
        "id": "V4QK8nAtmoMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTVMXgT1mp47",
        "outputId": "5da03eda-1270-4e55-f25f-6166c1bf4696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBQKvyxnme9A"
      },
      "outputs": [],
      "source": [
        "\n",
        "from retinaface import RetinaFace\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate brightness\n",
        "def calculate_brightness(img):\n",
        "    return np.mean(cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[:,:,2])\n",
        "\n",
        "# Function to calculate contrast\n",
        "def calculate_contrast(img):\n",
        "    return img.std()\n",
        "\n",
        "# Function to calculate noise\n",
        "def calculate_noise(img):\n",
        "    return np.var(img)\n",
        "\n",
        "# Function to calculate sharpness using Laplacian\n",
        "def calculate_sharpness(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "def calculate_blur(img):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "\n",
        "# Function to calculate the alignment of the face\n",
        "def calculate_alignment(landmarks):\n",
        "    left_eye = np.array(landmarks['left_eye'])\n",
        "    right_eye = np.array(landmarks['right_eye'])\n",
        "    delta_y = right_eye[1] - left_eye[1]\n",
        "    delta_x = right_eye[0] - left_eye[0]\n",
        "    angle = np.degrees(np.arctan2(delta_y, delta_x))\n",
        "    return abs(angle)\n",
        "def check_image_size(img, min_size):\n",
        "    height, width = img.shape[:2]\n",
        "    return width >= min_size['width'] and height >= min_size['height']\n",
        "\n",
        "# Function to check if the eyes, nose, and ears are parallel\n",
        "def are_features_parallel(face_info):\n",
        "    landmarks = face_info['landmarks']\n",
        "    eye_left = landmarks['left_eye']\n",
        "    eye_right = landmarks['right_eye']\n",
        "    nose = landmarks['nose']\n",
        "    ear_left = landmarks.get('left_ear', None)\n",
        "    ear_right = landmarks.get('right_ear', None)\n",
        "\n",
        "    # Check if eyes are parallel (difference in y-coordinates should be minimal)\n",
        "    eyes_parallel = abs(eye_left[1] - eye_right[1]) < 20  # Increased tolerance\n",
        "\n",
        "    # Check if nose is centered between the eyes\n",
        "    nose_centered = abs(nose[0] - (eye_left[0] + eye_right[0]) / 2) < 20  # Increased tolerance\n",
        "\n",
        "    # Check if ears are parallel (optional, depends on availability)\n",
        "    ears_parallel = True\n",
        "    if ear_left is not None and ear_right is not None:\n",
        "        ears_parallel = abs(ear_left[1] - ear_right[1]) < 20  # Increased tolerance\n",
        "\n",
        "    return eyes_parallel and nose_centered and ears_parallel\n",
        "\n",
        "\n",
        "def quality_assessment_check(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    faces = RetinaFace.detect_faces(img)\n",
        "\n",
        "    for face_id, face_info in faces.items():\n",
        "        x1, y1, x2, y2 = face_info['facial_area']\n",
        "        face_img = img[y1:y2, x1:x2]\n",
        "\n",
        "        # Calculate features\n",
        "        brightness = calculate_brightness(face_img)\n",
        "        contrast = calculate_contrast(face_img)\n",
        "        noise = calculate_noise(face_img)\n",
        "        sharpness = calculate_sharpness(face_img)\n",
        "        blur = calculate_blur(face_img)\n",
        "        alignment = calculate_alignment(face_info['landmarks'])\n",
        "\n",
        "        # print(f'Brightness: {brightness}, Contrast: {contrast}, Noise: {noise}, Sharpness: {sharpness}')\n",
        "        # Define threshold values\n",
        "        threshold_values = {\n",
        "            'brightness': 100,\n",
        "            'contrast': 60,\n",
        "            'noise': 3500,\n",
        "            'sharpness': 150,\n",
        "            'blur': 150,# Adjusted based on the Laplacian variance scale\n",
        "            'alignment': 180\n",
        "        }\n",
        "        min_size = {\n",
        "            'width': 1000,\n",
        "            'height': 2000\n",
        "        }\n",
        "        # Apply thresholds\n",
        "        # if brightness > 50 and contrast < 80 and noise < 5000 and sharpness < 1800:\n",
        "        # if brightness > 90 and contrast < 80 and noise < 5000 and sharpness < 150:\n",
        "            # img = cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            # filename = f'{person_folder}/photo_{accepted_images + 1}.jpg'\n",
        "            # cv2.imwrite(filename, img)\n",
        "            # print(f'quality_assessment_check passed.'\n",
        "        if (brightness > threshold_values['brightness'] and\n",
        "                        contrast < threshold_values['contrast'] and\n",
        "                        noise < threshold_values['noise'] and\n",
        "                        sharpness > threshold_values['sharpness'] and\n",
        "                        blur < threshold_values['blur']and\n",
        "                        alignment < threshold_values['alignment']):\n",
        "            check = True\n",
        "        else:\n",
        "            # print('quality_assessment_check failed.')\n",
        "            check = False\n",
        "\n",
        "    return face_img, check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxn1LnIUme9M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from deepface import DeepFace\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the LSTM model with output dimension 128\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Pack the sequence\n",
        "        packed_input = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, _) = self.lstm(packed_input)\n",
        "        # We only need the last hidden state\n",
        "        out = self.fc(hn[-1])\n",
        "        return out\n",
        "\n",
        "# Model parameters\n",
        "input_dim = 512  # Dimension of the Facenet512 embeddings\n",
        "hidden_dim = 256\n",
        "output_dim = 128  # Output dimension of the LSTM\n",
        "num_layers = 4  # Number of LSTM layers\n",
        "\n",
        "# Instantiate the model\n",
        "model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDPHAiSime9O"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import cv2\n",
        "# from deepface import DeepFace\n",
        "# import matplotlib.pyplot as plt\n",
        "# from collections import defaultdict  # Add this import\n",
        "\n",
        "# # Block 1: Code for Loading Class Embeddings and Rounding Off\n",
        "# lstm_no_aug_path = \"/content/drive/MyDrive/emb_Data/lstm_no_prep.pth\"\n",
        "# model.load_state_dict(torch.load(lstm_no_aug_path))\n",
        "# model.eval()\n",
        "# #check\n",
        "# def normalize_brightness_contrast_grayscale(image):\n",
        "#     # Normalize brightness using LAB color space\n",
        "#     lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "#     l, a, b = cv2.split(lab)\n",
        "#     l = cv2.equalizeHist(l)\n",
        "#     lab = cv2.merge((l, a, b))\n",
        "#     normalized_image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "#     # Contrast normalization using CLAHE\n",
        "#     lab = cv2.cvtColor(normalized_image, cv2.COLOR_BGR2LAB)\n",
        "#     l, a, b = cv2.split(lab)\n",
        "#     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "#     l = clahe.apply(l)\n",
        "#     lab = cv2.merge((l, a, b))\n",
        "#     normalized_contrast_image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "#     # Convert to grayscale\n",
        "#     gray_image = cv2.cvtColor(normalized_contrast_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#     return gray_image\n",
        "\n",
        "\n",
        "\n",
        "# # Function to extract features and normalize all images in a folder\n",
        "# def process_class_images(class_folder):\n",
        "#     img_paths = [os.path.join(class_folder, img) for img in os.listdir(class_folder)]\n",
        "#     brightness_values = []\n",
        "#     contrast_values = []\n",
        "\n",
        "#     # Calculate brightness and contrast for each image\n",
        "#     for img_path in img_paths:\n",
        "#         image = cv2.imread(img_path)\n",
        "#         brightness, contrast = calculate_image_features(image)\n",
        "#         brightness_values.append(brightness)\n",
        "#         contrast_values.append(contrast)\n",
        "\n",
        "#     # Get average brightness and contrast for the class\n",
        "#     avg_brightness = np.mean(brightness_values)\n",
        "#     avg_contrast = np.mean(contrast_values)\n",
        "\n",
        "#     print(f\"Class {class_folder}: Avg Brightness = {avg_brightness}, Avg Contrast = {avg_contrast}\")\n",
        "\n",
        "#     processed_images = []\n",
        "#     for img_path in img_paths:\n",
        "#         image = cv2.imread(img_path)\n",
        "#         # Normalize using average values of brightness and contrast\n",
        "#         processed_image = normalize_with_custom_values(image, avg_brightness, avg_contrast)\n",
        "#         processed_images.append(processed_image)\n",
        "\n",
        "#     return processed_images\n",
        "\n",
        "# # Block 2: Code for Getting Image Embedding\n",
        "# def get_initial_image_embedding(image):\n",
        "#     # if type(image_path) == \"str\":\n",
        "#     #     image = cv2.imread(image_path)\n",
        "#     # else:\n",
        "#     #     image = image_path\n",
        "\n",
        "#     if len(image.shape) == 2:\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "#     processed_image = normalize_brightness_contrast_grayscale(image)\n",
        "#     if len(processed_image.shape) == 2:\n",
        "#         processed_image = cv2.cvtColor(processed_image, cv2.COLOR_GRAY2RGB)\n",
        "#     embedding = DeepFace.represent(img_path=processed_image, model_name=\"Facenet512\", detector_backend='skip')\n",
        "#     embarr = np.array(embedding[0][\"embedding\"])\n",
        "#     return torch.tensor(embarr, dtype=torch.float32).unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "# def get_final_image_embedding(image_path, model):\n",
        "#     initial_embedding = get_initial_image_embedding(image_path)\n",
        "#     lengths = torch.tensor([initial_embedding.shape[1]])  # Sequence length is 1\n",
        "#     with torch.no_grad():\n",
        "#         final_embedding = model(initial_embedding, lengths)\n",
        "#     return final_embedding\n",
        "\n",
        "# # Block 3: Code for Comparing New Image Embedding\n",
        "# def load_all_embeddings(save_dir):\n",
        "#     all_embeddings = defaultdict(list)\n",
        "#     for file in os.listdir(save_dir):\n",
        "#         if file.endswith('.npy'):\n",
        "#             class_num = int(file.split('_')[1])\n",
        "#             vector = np.load(os.path.join(save_dir, file))\n",
        "#             all_embeddings[class_num].append(torch.tensor(vector, dtype=torch.float32))\n",
        "#     return all_embeddings\n",
        "\n",
        "# def compare_with_tolerance(embedding1, embedding2, tolerance=0.00025):\n",
        "#     return torch.sum(torch.abs(embedding1 - embedding2) <= tolerance).item()\n",
        "\n",
        "# def result(image_path, model, save_dir=None, tolerance=0.00025):\n",
        "#     new_image_final_embedding = get_final_image_embedding(image_path, model)\n",
        "#     # all_embeddings = load_all_embeddings(save_dir)\n",
        "\n",
        "#     new_image_final_embedding_rounded = torch.round(new_image_final_embedding)\n",
        "\n",
        "#     # print(f\"New Image Embedding:\\n{new_image_final_embedding_rounded.squeeze().numpy()}\")\n",
        "#     return new_image_final_embedding_rounded.squeeze().numpy()\n",
        "\n",
        "# # ------\n",
        "# # This function will compare each image with every other image in the folder\n",
        "# def matching_precentage(lis, tolerance=0.1):\n",
        "#     final_lst = []\n",
        "#     for start in range(len(lis)):\n",
        "#         for end in range(len(lis[start:-1])):\n",
        "#             # print(start, start+end+1)\n",
        "#             emb = np.sum(abs(lis[start] - lis[start+end+1]) <= tolerance).item()\n",
        "#             # print(emb)\n",
        "#             final_lst.append(emb)\n",
        "\n",
        "#     count = 0\n",
        "#     for i in final_lst:\n",
        "#         if i == 128:\n",
        "#             count += 1\n",
        "\n",
        "#     if len(final_lst) == 0:\n",
        "#         matching_percentage = None\n",
        "#     else:   matching_percentage = int((count/len(final_lst))*100)\n",
        "\n",
        "#     print(f\"Matching Percentage: {matching_percentage}%\")\n",
        "#     print(f\"Total Matching Images: {count} out of {len(final_lst)} pairs\")\n",
        "#     return matching_percentage, final_lst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX-phgLcme9Q"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "# import glob\n",
        "\n",
        "# # dir = \"/Users/kartikkhandelwal/IITH/IITH-Soulverse/Latest/Dataset/Ankit\"\n",
        "# # parent_dir = \"/Users/kartikkhandelwal/IITH/IITH-Soulverse/Latest/Dataset\"\n",
        "# parent_dir = \"/content/drive/MyDrive/New test\"\n",
        "\n",
        "# for dir in glob.glob(parent_dir + \"/*\"):\n",
        "#     lis = []\n",
        "#     qac_img = 0\n",
        "#     name = dir.split(\"/\")[-1]\n",
        "#     print(name)\n",
        "#     # for i in tqdm(range(len(os.listdir(dir)) - 1)):\n",
        "#     try:\n",
        "#         for new_image_path in glob.glob(dir + \"/*\"):\n",
        "#             cropped_img, check = quality_assessment_check(new_image_path)\n",
        "#             if check:\n",
        "#                 qac_img = qac_img + 1\n",
        "#                 emb = result(cropped_img, model)\n",
        "#                 lis.append(emb)\n",
        "\n",
        "#         per, matching = matching_precentage(lis)\n",
        "#         print(f\"Quality Assessment Check Passed: {qac_img} out of {len(os.listdir(dir))}\")\n",
        "#         print(matching)\n",
        "#         print(\"-\"*25)\n",
        "#     except Exception as e:\n",
        "#         print(e)\n",
        "#         print(new_image_path)\n",
        "#         print(\"-\"*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------"
      ],
      "metadata": {
        "id": "b3RximAyrbzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from deepface import DeepFace\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "# Load LSTM model weights\n",
        "def load_model_weights(model, weight_path):\n",
        "    model.load_state_dict(torch.load(weight_path))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Function to calculate brightness and contrast\n",
        "def calculate_brightness_contrast(image):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    brightness = np.mean(gray_image)\n",
        "    contrast = gray_image.std()\n",
        "    return brightness, contrast\n",
        "\n",
        "# Function to normalize images with average brightness and contrast\n",
        "def normalize_image(image, avg_brightness, avg_contrast):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    current_brightness, current_contrast = calculate_brightness_contrast(image)\n",
        "\n",
        "    # Adjust brightness\n",
        "    brightness_adjustment = avg_brightness - current_brightness\n",
        "    adjusted_image = gray_image + brightness_adjustment\n",
        "    adjusted_image = np.clip(adjusted_image, 0, 255)\n",
        "\n",
        "    # Adjust contrast\n",
        "    contrast_adjustment = avg_contrast / current_contrast if current_contrast != 0 else 1\n",
        "    adjusted_image = (adjusted_image - avg_brightness) * contrast_adjustment + avg_brightness\n",
        "    adjusted_image = np.clip(adjusted_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return cv2.cvtColor(adjusted_image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "# Process images in class folder to calculate average brightness/contrast and normalize them\n",
        "def process_class_images(class_folder):\n",
        "    img_paths = [os.path.join(class_folder, img) for img in os.listdir(class_folder)]\n",
        "    brightness_values = []\n",
        "    contrast_values = []\n",
        "\n",
        "    # Calculate brightness and contrast for each image\n",
        "    for img_path in img_paths:\n",
        "        image = cv2.imread(img_path)\n",
        "        brightness, contrast = calculate_brightness_contrast(image)\n",
        "        brightness_values.append(brightness)\n",
        "        contrast_values.append(contrast)\n",
        "\n",
        "    # Average brightness and contrast for the class\n",
        "    avg_brightness = np.mean(brightness_values)\n",
        "    avg_contrast = np.mean(contrast_values)\n",
        "    print(f\"Class {class_folder}: Avg Brightness = {avg_brightness}, Avg Contrast = {avg_contrast}\")\n",
        "\n",
        "    processed_images = []\n",
        "    for img_path in img_paths:\n",
        "        image = cv2.imread(img_path)\n",
        "        processed_image = normalize_image(image, avg_brightness, avg_contrast)\n",
        "        processed_images.append(processed_image)\n",
        "\n",
        "    return processed_images\n",
        "\n",
        "# Get embedding from an image using DeepFace\n",
        "def get_image_embedding(image, model):\n",
        "    if len(image.shape) == 2:  # Convert grayscale to RGB if necessary\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    embedding = DeepFace.represent(img_path=image, model_name=\"Facenet512\", detector_backend='skip')\n",
        "    embarr = np.array(embedding[0][\"embedding\"])\n",
        "    return torch.tensor(embarr, dtype=torch.float32).unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "# Extract final image embedding by passing through LSTM model\n",
        "def get_final_image_embedding(image, model):\n",
        "    initial_embedding = get_image_embedding(image, model)\n",
        "    lengths = torch.tensor([initial_embedding.shape[1]])  # Sequence length\n",
        "    with torch.no_grad():\n",
        "        final_embedding = model(initial_embedding, lengths)\n",
        "    return final_embedding\n",
        "\n",
        "# Load all embeddings from a directory (optional for saving embeddings)\n",
        "def load_all_embeddings(save_dir):\n",
        "    all_embeddings = defaultdict(list)\n",
        "    for file in os.listdir(save_dir):\n",
        "        if file.endswith('.npy'):\n",
        "            class_num = int(file.split('_')[1])\n",
        "            vector = np.load(os.path.join(save_dir, file))\n",
        "            all_embeddings[class_num].append(torch.tensor(vector, dtype=torch.float32))\n",
        "    return all_embeddings\n",
        "\n",
        "# Compare embeddings with a tolerance value\n",
        "def compare_with_tolerance(embedding1, embedding2, tolerance=0.00025):\n",
        "    return torch.sum(torch.abs(embedding1 - embedding2) <= tolerance).item()\n",
        "\n",
        "# Function to get results for image embeddings\n",
        "def result(image_path, model, tolerance=0.00025):\n",
        "    new_image_embedding = get_final_image_embedding(image_path, model)\n",
        "    new_image_embedding_rounded = torch.round(new_image_embedding)\n",
        "    return new_image_embedding_rounded.squeeze().numpy()\n",
        "\n",
        "# Calculate matching percentage between embeddings\n",
        "def matching_percentage(embeddings_list, tolerance=0.1):\n",
        "    final_lst = []\n",
        "    for start in range(len(embeddings_list)):\n",
        "        for end in range(len(embeddings_list[start:-1])):\n",
        "            emb = np.sum(abs(embeddings_list[start] - embeddings_list[start+end+1]) <= tolerance).item()\n",
        "            final_lst.append(emb)\n",
        "\n",
        "    count = 0\n",
        "    for i in final_lst:\n",
        "        if i == 128:\n",
        "            count += 1\n",
        "\n",
        "    matching_percentage = int((count / len(final_lst)) * 100) if len(final_lst) > 0 else None\n",
        "    print(f\"Matching Percentage: {matching_percentage}%\")\n",
        "    print(f\"Total Matching Images: {count} out of {len(final_lst)} pairs\")\n",
        "    return matching_percentage, final_lst\n",
        "\n",
        "# Main loop for processing images in directories\n",
        "parent_dir = \"/content/drive/MyDrive/New test\"\n",
        "\n",
        "for dir in glob.glob(parent_dir + \"/*\"):\n",
        "    embeddings_list = []\n",
        "    qac_img = 0\n",
        "    name = dir.split(\"/\")[-1]\n",
        "    print(name)\n",
        "\n",
        "    try:\n",
        "        for image_path in glob.glob(dir + \"/*\"):\n",
        "            cropped_img, check = quality_assessment_check(image_path)\n",
        "            if check:\n",
        "                qac_img += 1\n",
        "                embedding = result(cropped_img, model)\n",
        "                embeddings_list.append(embedding)\n",
        "\n",
        "        percentage, matching = matching_percentage(embeddings_list)\n",
        "        print(f\"Quality Assessment Check Passed: {qac_img} out of {len(os.listdir(dir))}\")\n",
        "        print(matching)\n",
        "        print(\"-\" * 25)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(image_path)\n",
        "        print(\"-\" * 25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgqXLnF3rSQ3",
        "outputId": "4dceafe8-2ea5-4c5e-a221-45b2306ff702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ankit \n",
            "Matching Percentage: None%\n",
            "Total Matching Images: 0 out of 0 pairs\n",
            "Quality Assessment Check Passed: 0 out of 45\n",
            "[]\n",
            "-------------------------\n",
            "Arpit\n",
            "Matching Percentage: None%\n",
            "Total Matching Images: 0 out of 0 pairs\n",
            "Quality Assessment Check Passed: 0 out of 36\n",
            "[]\n",
            "-------------------------\n",
            "Pabitra_sir\n",
            "Matching Percentage: None%\n",
            "Total Matching Images: 0 out of 0 pairs\n",
            "Quality Assessment Check Passed: 0 out of 29\n",
            "[]\n",
            "-------------------------\n",
            "VISHNU\n",
            "Matching Percentage: None%\n",
            "Total Matching Images: 0 out of 0 pairs\n",
            "Quality Assessment Check Passed: 0 out of 37\n",
            "[]\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDn0tQI1rSOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz3LUxj0me9T",
        "outputId": "8440cbc0-3871-457a-9569-293374e519a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy1mm_4tme9U",
        "outputId": "b09503b3-30c5-4de2-c0ee-4b12d538d06e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "lis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG9RRG66me9W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv-soulscan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
